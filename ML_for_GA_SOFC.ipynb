{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/9jgm0zsUkx+cv5wZQWyb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzrd91/Machine-Learning-Approaches-for-Static-Modeling-of-SOFC-Using-GP-and-ANN/blob/main/ML_for_GA_SOFC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1JlM2eNIA3o",
        "outputId": "69558421-d60a-4dfb-ee83-2d04aea1f28c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.25.2)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import operator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import regularizers\n",
        "from deap import algorithms, base, creator, tools, gp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "s1Nfn5M4H3vN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters used in the SOFC stack\n",
        "number_of_stacks = 1\n",
        "temperature_values = [873, 973, 1073, 1173, 1273] # List of temperature values in Kelvin\n",
        "E0 = 1.18 # V\n",
        "N0 = 384\n",
        "u = 0.8\n",
        "Kr = 0.993e-3 # mol/(s A)\n",
        "Kr_cell = Kr / N0\n",
        "KH2 = 0.843 # mol/(s atm)\n",
        "KH2O = 0.281 # mol/(s atm)\n",
        "KO2 = 2.52 # mol/(s atm)\n",
        "r = 0.126 # U\n",
        "rcell = r / N0\n",
        "rHO = 1.145\n",
        "i0_den = 20 # mA/cm2\n",
        "ilimit_den = 900 # mA/cm2\n",
        "A = 1000 # cm2\n",
        "n = 2\n",
        "\n",
        "F = 96485\n",
        "\n",
        "alpha = 0.5\n",
        "R = 0.0821 # atm/(mol K)\n",
        "N = 20\n",
        "\n",
        "PH2 = 1.265\n",
        "PO2 = 2.527\n",
        "PH2O = 0.467"
      ],
      "metadata": {
        "id": "quH7Q9noIH3p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_statistics(data):\n",
        "    best = np.min(data)\n",
        "    worst = np.max(data)\n",
        "    mean = np.mean(data)\n",
        "    variance = np.var(data)\n",
        "    return best, worst, mean, variance\n",
        "\n",
        "def evaluate(individual, x_train, y_train, x_test, y_test):\n",
        "    func = gp.compile(expr=individual, pset=pset)\n",
        "    y_pred_train = [func(T, current) for T, current in x_train]\n",
        "    y_pred_test = [func(T, current) for T, current in x_test]\n",
        "    mae_train = np.mean(np.abs(np.array(y_pred_train) - y_train))\n",
        "    mae_test = np.mean(np.abs(np.array(y_pred_test) - y_test))\n",
        "    mse_train = np.mean((np.array(y_pred_train) - y_train) ** 2)\n",
        "    mse_test = np.mean((np.array(y_pred_test) - y_test) ** 2)\n",
        "    return mae_train, mse_train, mae_test, mse_test\n",
        "\n",
        "def generate_data(num_samples=1000):\n",
        "    current_range = range(20, 900)\n",
        "    train_currents = random.sample(current_range, int(0.8 * len(current_range))) # 80% for training\n",
        "    test_currents = random.sample(current_range, int(0.2 * len(current_range))) # 20% for testing\n",
        "    X_train, Y_train, X_test, Y_test = [], [], [], []\n",
        "\n",
        "    for T in temperature_values:\n",
        "        for current in train_currents:\n",
        "            # Introduce randomness by perturbing temperature and current\n",
        "            perturbed_T = T + random.uniform(-1, 1) # Add random noise to temperature\n",
        "            perturbed_current = current + random.uniform(-10, 10) # Add random noise to current\n",
        "\n",
        "            # Use the perturbed values in your calculations\n",
        "            Enernst = E0 + (((R * perturbed_T) / (2 * F)) * (2.303 * (math.log((PH2 * PO2) / (PH2O)))))\n",
        "            i_limit = ilimit_den / A\n",
        "            activation_loss = ((R * perturbed_T) / (alpha * n * F)) * (2.303 * (math.log((i_limit / i0_den))))\n",
        "            concentration_loss = - (((R * perturbed_T) / (n * F)) * (2.303 * (math.log(1 - (i_limit / ilimit_den)))))\n",
        "            VStack = Enernst - activation_loss - concentration_loss\n",
        "\n",
        "            X_train.append((perturbed_T, perturbed_current))\n",
        "            Y_train.append(VStack)\n",
        "\n",
        "        for current in test_currents:\n",
        "            # Introduce randomness by perturbing temperature and current\n",
        "            perturbed_T = T + random.uniform(-1, 1) # Add random noise to temperature\n",
        "            perturbed_current = current + random.uniform(-10, 10) # Add random noise to current\n",
        "\n",
        "            # Use the perturbed values in your calculations\n",
        "            Enernst = E0 + (((R * perturbed_T) / (2 * F)) * (2.303 * (math.log((PH2 * PO2) / (PH2O)))))\n",
        "            i_limit = ilimit_den / A\n",
        "            activation_loss = ((R * perturbed_T) / (alpha * n * F)) * (2.303 * (math.log((i_limit / i0_den))))\n",
        "            concentration_loss = - (((R * perturbed_T) / (n * F)) * (2.303 * (math.log(1 - (i_limit / ilimit_den)))))\n",
        "            VStack = Enernst - activation_loss - concentration_loss\n",
        "\n",
        "            X_test.append((perturbed_T, perturbed_current))\n",
        "            Y_test.append(VStack)\n",
        "\n",
        "        if len(X_train) >= num_samples:\n",
        "            break\n",
        "\n",
        "    random.shuffle(X_train)\n",
        "    random.shuffle(X_test)\n",
        "    random.shuffle(Y_train)\n",
        "    random.shuffle(Y_test)\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "pset = gp.PrimitiveSet(\"MAIN\", arity=2)\n",
        "pset.addPrimitive(operator.add, 2)\n",
        "pset.addPrimitive(operator.sub, 2)\n",
        "pset.addPrimitive(operator.mul, 2)\n",
        "pset.addPrimitive(math.sin, 1)\n",
        "pset.addPrimitive(math.cos, 1)\n",
        "pset.addTerminal(1.0)\n",
        "pset.addEphemeralConstant(\"rand\", lambda: random.uniform(-1, 1))\n",
        "\n",
        "max_depth = 3\n",
        "\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0, -1.0, -1.0, -1.0))\n",
        "creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMin)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=2)\n",
        "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"mate\", gp.cxOnePoint)\n",
        "toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=2)\n",
        "toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr_mut, pset=pset)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=max_depth)\n",
        "\n",
        "random.seed(142)\n",
        "\n",
        "runs = 50\n",
        "pop_size = 100\n",
        "crossover_prob = 0.8\n",
        "mutation_prob = 0.01\n",
        "num_generations = 50\n",
        "\n",
        "mae_train_list, mae_test_list = [], []\n",
        "mse_train_list, mse_test_list = [] , []\n",
        "\n",
        "best_individuals, best_equations = [], []\n",
        "\n",
        "for run in range(runs):\n",
        "    X_train, Y_train, X_test, Y_test = generate_data()\n",
        "    toolbox.register(\"evaluate\", evaluate, x_train=X_train, y_train=Y_train, x_test=X_test, y_test=Y_test)\n",
        "\n",
        "    population = toolbox.population(n=pop_size)\n",
        "    hof = tools.HallOfFame(1)\n",
        "\n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats.register(\"min_mae_train\", np.min)\n",
        "    stats.register(\"min_mse_train\", np.min)\n",
        "    stats.register(\"min_mae_test\", np.min)\n",
        "    stats.register(\"min_mse_test\", np.min)\n",
        "\n",
        "    algorithms.eaMuPlusLambda(population, toolbox, mu=pop_size, lambda_=2 * pop_size, cxpb=crossover_prob, mutpb=mutation_prob, ngen=num_generations, stats=stats, halloffame=hof, verbose=False)\n",
        "\n",
        "    best_individual = hof[0]\n",
        "    best_individuals.append(best_individual)\n",
        "    mae_train, mse_train, mae_test, mse_test = best_individual.fitness.values\n",
        "\n",
        "    mae_train_list.append(mae_train)\n",
        "    mse_train_list.append(mse_train)\n",
        "    mae_test_list.append(mae_test)\n",
        "    mse_test_list.append(mse_test)\n",
        "\n",
        "    best_individual_idx = np.argmin(mae_test_list)\n",
        "    best_individual_test = best_individuals[best_individual_idx]\n",
        "\n",
        "    best_equation = str(best_individual_test)\n",
        "    best_equations.append(best_equation)\n",
        "\n",
        "all_mae_train, all_mse_train = np.array(mae_train_list), np.array(mse_train_list)\n",
        "all_mae_test, all_mse_test = np.array(mae_test_list), np.array(mse_test_list)\n",
        "\n",
        "best_mae_train, worst_mae_train, mean_mae_train, variance_mae_train = calculate_statistics(all_mae_train)\n",
        "best_mse_train, worst_mse_train, mean_mse_train, variance_mse_train = calculate_statistics(all_mse_train)\n",
        "\n",
        "best_mae_test, worst_mae_test, mean_mae_test, variance_mae_test = calculate_statistics(all_mae_test)\n",
        "best_mse_test, worst_mse_test, mean_mse_test, variance_mse_test = calculate_statistics(all_mse_test)\n",
        "\n",
        "best_individual_test = best_individuals[best_individual_idx]\n",
        "best_equation = str(best_individual_test)\n",
        "compiled_function = gp.compile(expr=best_individual_test, pset=pset)\n",
        "\n",
        "best_result = None\n",
        "best_T_value = None\n",
        "best_current_value = None\n",
        "\n",
        "for T_value, current_value in zip(X_test, Y_test):\n",
        "    result = compiled_function(T_value, current_value)\n",
        "    if best_result is None or result < best_result:\n",
        "        best_result = result\n",
        "        best_T_value = T_value\n",
        "        best_current_value = current_value\n",
        "\n",
        "print(\"MAE on Training Data \\n\\n Best: {}\\n Worst: {}\\n Mean: {}\\n Variance: {}\\n\".format(best_mae_train, worst_mae_train, mean_mae_train, variance_mae_train))\n",
        "print(\"MSE on Training Data \\n\\n Best: {}\\n Worst: {}\\n Mean: {}\\n Variance: {}\\n\".format(best_mse_train, worst_mse_train, mean_mse_train, variance_mse_train))\n",
        "\n",
        "print(\"MAE on Test Data \\n\\n Best: {}\\n Worst: {}\\n Mean: {}\\n Variance: {}\".format(best_mae_test, worst_mae_test, mean_mae_test, variance_mae_test))\n",
        "print(\"\\nMSE on Test Data \\n\\n Best: {}\\n Worst: {}\\n Mean: {}\\n Variance: {}\".format(best_mse_test, worst_mse_test, mean_mse_test, variance_mse_test))\n",
        "\n",
        "print(\"\\n Best Value (with respect to test data): {}\\n Best Equation (with respect to test data): {}\\n\".format(best_result, best_equations[np.argmin(mae_test_list)]))\n"
      ],
      "metadata": {
        "id": "Zqj2NpUwITst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Artificial Neural Network\n",
        "\n",
        "X_train, Y_train, X_test, Y_test = generate_data()\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
        "train_mae_list, train_mse_list, test_mae_list, test_mse_list = [], [], [], []\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(2,)), # Input layer with 2 features (temperature and current)\n",
        "    keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)), # L2 regularization\n",
        "    keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)), # L2 regularization\n",
        "    keras.layers.Dense(1) # Output layer with a single neuron for regression\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_val, Y_val), verbose=0)\n",
        "\n",
        "train_loss, train_mae = model.evaluate(X_train, Y_train, verbose=0)\n",
        "train_mae_list.append(train_mae)\n",
        "train_mse_list.append(train_loss)\n",
        "\n",
        "test_loss, test_mae = model.evaluate(X_test, Y_test, verbose=0)\n",
        "test_mae_list.append(test_mae)\n",
        "test_mse_list.append(test_loss)\n",
        "\n",
        "Y_train_pred = model.predict(X_train)\n",
        "train_mse = mean_squared_error(Y_train, Y_train_pred)\n",
        "train_mse_list.append(train_mse)\n",
        "\n",
        "Y_test_pred = model.predict(X_test)\n",
        "test_mse = mean_squared_error(Y_test, Y_test_pred)\n",
        "test_mse_list.append(test_mse)\n",
        "\n",
        "weights = model.get_weights()\n",
        "\n",
        "T = 900\n",
        "current = 500\n",
        "\n",
        "input_layer = np.array([T, current])\n",
        "\n",
        "def get_value(T, current):\n",
        "    # Use the weights and biases to construct the equation\n",
        "    hidden_layer1 = np.dot(input_layer, weights[0]) + weights[1]\n",
        "    hidden_layer1 = np.maximum(0, hidden_layer1) # ReLU activation\n",
        "    hidden_layer2 = np.dot(hidden_layer1, weights[2]) + weights[3]\n",
        "    hidden_layer2 = np.maximum(0, hidden_layer2) # ReLU activation\n",
        "    output = np.dot(hidden_layer2, weights[4]) + weights[5]\n",
        "    return output\n",
        "\n",
        "def construct_equation():\n",
        "    equation = f\"output = ReLU({weights[0][0]} * T + {weights[1][0]}) # First hidden layer\\n\"\n",
        "    equation += f\"output = ReLU({weights[2][0]} * output + {weights[3][0]}) # Second hidden layer\\n\"\n",
        "    equation += f\"output = {weights[4][0]} * output + {weights[5][0]} # Output layer\"\n",
        "    return equation\n",
        "\n",
        "print(\"MAE on Training Data \\n\\nBest: {}\\nWorst: {}\\nMean: {} \\nVariance: {}\\n\".format(*calculate_statistics(train_mae_list)))\n",
        "print(\"MSE on Training Data \\n\\nBest: {}\\nWorst: {}\\nMean: {} \\nVariance: {}\\n\".format(*calculate_statistics(train_mse_list)))\n",
        "print(\"MAE on Testing Data \\n\\nBest: {}\\nWorst: {}\\nMean: {} \\nVariance: {}\\n\".format(*calculate_statistics(test_mae_list)))\n",
        "print(\"MSE on Testing Data \\n\\nBest: {}\\nWorst: {}\\nMean: {} \\nVariance: {}\\n\".format(*calculate_statistics(test_mse_list)))\n",
        "\n",
        "print(\"\\nBest Value (with respect to test data): {}\".format(str(get_value(T, current)[0])))\n",
        "\n",
        "equation = construct_equation()\n",
        "print(\"\\nBest Equation (with respect to test data): {}\".format(construct_equation()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfyOfAx7lTDi",
        "outputId": "3f6563e8-0a17-4529-f5d2-2f28297fa007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36/36 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "MAE on Training Data \n",
            "\n",
            "Best: 0.08477642387151718\n",
            "Worst: 0.08477642387151718\n",
            "Mean: 0.08477642387151718 \n",
            "Variance: 0.0\n",
            "\n",
            "MSE on Training Data \n",
            "\n",
            "Best: 0.010958548096140807\n",
            "Worst: 0.21881218254566193\n",
            "Mean: 0.11488536532090136 \n",
            "Variance: 0.010800783338468789\n",
            "\n",
            "MAE on Testing Data \n",
            "\n",
            "Best: 0.08796335011720657\n",
            "Worst: 0.08796335011720657\n",
            "Mean: 0.08796335011720657 \n",
            "Variance: 0.0\n",
            "\n",
            "MSE on Testing Data \n",
            "\n",
            "Best: 0.011814823093036659\n",
            "Worst: 0.21966834366321564\n",
            "Mean: 0.11574158337812615 \n",
            "Variance: 0.010800771503354453\n",
            "\n",
            "\n",
            "Best Value (with respect to test data): 1.0639705824631775\n",
            "\n",
            "Best Equation (with respect to test data): output = ReLU([ 1.23343244e-01 -1.39940143e-01  1.48459494e-01 -4.42455018e-34\n",
            " -7.42889866e-02 -4.25265096e-02 -4.34854137e-15  9.92128849e-02\n",
            " -2.15619380e-08  2.94536382e-01  1.44217595e-01 -9.29116011e-02\n",
            "  1.09519735e-01  1.75676770e-34  1.92743793e-01  2.15824246e-01\n",
            "  2.46098340e-01  4.72209133e-28 -1.20949681e-10  6.21991698e-03\n",
            "  3.42115830e-03  8.81288275e-02 -2.62943469e-03  2.90576739e-34\n",
            "  7.60706514e-02  2.24127203e-01  2.69365403e-34  1.82937935e-01\n",
            "  6.39743581e-02  1.92810386e-01 -1.89585999e-01  1.11749873e-01\n",
            "  2.55219907e-01  2.39564344e-01  1.70697466e-01 -2.14268826e-03\n",
            "  1.88705549e-02  1.51675731e-01 -1.07789874e-01  1.55549079e-01\n",
            "  1.34644344e-01 -5.84157510e-03  5.84553366e-34  2.34297529e-01\n",
            "  1.98279038e-01  2.55557634e-02  1.36274561e-01  1.52684137e-01\n",
            "  1.32966056e-01 -1.08493403e-01  2.41701126e-01  1.07503682e-01\n",
            "  8.15487877e-02 -1.61823645e-01 -2.60200240e-02 -6.28006309e-02\n",
            "  2.33333096e-01 -1.93891011e-03  2.27552220e-01 -1.01366542e-01\n",
            " -5.68653690e-04 -2.76122039e-34 -4.07365868e-34  2.68037289e-01] * T + -0.001098350971005857) # First hidden layer\n",
            "output = ReLU([ 2.3036815e-01  1.8779524e-01 -1.1598179e-01  2.2410314e-01\n",
            " -1.0151438e-03 -1.2673024e-03 -8.5272705e-03  1.7478286e-01\n",
            " -1.0798331e-01 -2.2014253e-01 -2.5339391e-02 -2.6394892e-02\n",
            "  6.9879487e-02 -1.6459610e-03 -2.3833640e-02 -5.2918058e-02\n",
            " -2.1818409e-02 -8.3389007e-02  1.8773982e-01 -9.1129841e-08\n",
            " -3.4862132e-06 -1.5669798e-02  6.6602707e-02 -2.0106487e-02\n",
            "  1.1983769e-03 -1.5654713e-01  1.9537260e-01  2.2461237e-01\n",
            "  2.1140008e-01  2.2124059e-01 -1.9510858e-07  1.7758626e-01] * output + -0.010089358314871788) # Second hidden layer\n",
            "output = [-0.18587655] * output + 0.009872481226921082 # Output layer\n"
          ]
        }
      ]
    }
  ]
}